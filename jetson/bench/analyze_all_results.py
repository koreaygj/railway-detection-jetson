#!/usr/bin/env python3
"""
ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì¢…í•© ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
ì—¬ëŸ¬ ëª¨ë¸ì˜ ì„±ëŠ¥ê³¼ ì •í™•ë„ë¥¼ ë¹„êµí•˜ê³  ì‹œê°í™”

ì‚¬ìš©ë²•:
    python analyze_all_results.py ./comprehensive_results/ --save-report
    python analyze_all_results.py ./comprehensive_results/ --export-csv --plot
"""

import os
import sys
import json
import argparse
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import font_manager

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = ['DejaVu Sans', 'Arial Unicode MS', 'AppleGothic']
plt.rcParams['axes.unicode_minus'] = False

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ResultAnalyzer:
    def __init__(self, results_dir: str):
        """
        ê²°ê³¼ ë¶„ì„ê¸° ì´ˆê¸°í™”

        Args:
            results_dir: ê²°ê³¼ íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬
        """
        self.results_dir = Path(results_dir)
        self.results_data = []
        self.summary_df = None

        logger.info(f"ğŸ” ê²°ê³¼ ë¶„ì„ê¸° ì´ˆê¸°í™”: {self.results_dir}")



    def load_all_results(self, pattern: str = "*.json") -> int:
        """ëª¨ë“  ê²°ê³¼ íŒŒì¼ ë¡œë“œ"""
        result_files = list(self.results_dir.glob(pattern))

        if not result_files:
            logger.warning(f"âŒ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.results_dir}/{pattern}")
            return 0

        logger.info(f"ğŸ“ {len(result_files)}ê°œ ê²°ê³¼ íŒŒì¼ ë°œê²¬")

        for result_file in result_files:
            try:
                with open(result_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                # ë©”íƒ€ ì •ë³´ ì¶”ê°€
                data['_file_info'] = {
                    'filename': result_file.name,
                    'filepath': str(result_file),
                    'size': result_file.stat().st_size,
                    'modified': result_file.stat().st_mtime
                }

                self.results_data.append(data)
                logger.info(f"âœ… ë¡œë“œ: {result_file.name}")

            except Exception as e:
                logger.warning(f"âš ï¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {result_file.name} - {e}")

        logger.info(f"ğŸ“Š ì´ {len(self.results_data)}ê°œ ê²°ê³¼ ë¡œë“œ ì™„ë£Œ")
        return len(self.results_data)

    def extract_summary_data(self) -> pd.DataFrame:
        """ê²°ê³¼ ë°ì´í„°ì—ì„œ ìš”ì•½ ì •ë³´ ì¶”ì¶œ"""
        summary_data = []

        for data in self.results_data:
            try:
                summary = self._extract_single_result(data)
                if summary:
                    summary_data.append(summary)
            except Exception as e:
                logger.warning(f"ê²°ê³¼ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

        if not summary_data:
            logger.error("âŒ ì¶”ì¶œ ê°€ëŠ¥í•œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()

        df = pd.DataFrame(summary_data)

        # ì •ë ¬
        df = df.sort_values(['model_type', 'model_name'])

        self.summary_df = df
        logger.info(f"ğŸ“‹ ìš”ì•½ í…Œì´ë¸” ìƒì„± ì™„ë£Œ: {len(df)} í–‰")
        return df

    def _extract_single_result(self, data: Dict) -> Optional[Dict]:
        """ë‹¨ì¼ ê²°ê³¼ì—ì„œ ì •ë³´ ì¶”ì¶œ"""
        summary = {}

        # íŒŒì¼ ì •ë³´
        file_info = data.get('_file_info', {})
        summary['filename'] = file_info.get('filename', 'unknown')

        # ëª¨ë¸ ì •ë³´
        if 'benchmark_info' in data:
            # comprehensive_benchmark ê²°ê³¼
            model_info = data['benchmark_info']
            summary['model_name'] = Path(model_info.get('model_name', 'unknown')).stem
            summary['model_path'] = model_info.get('model_path', 'unknown')
            summary['timestamp'] = model_info.get('timestamp', 'unknown')
        elif 'model_info' in data:
            # ê°œë³„ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
            model_info = data['model_info']
            summary['model_name'] = Path(model_info.get('file_name', model_info.get('model_name', 'unknown'))).stem
            summary['model_path'] = model_info.get('file_path', model_info.get('model_path', 'unknown'))
            summary['timestamp'] = data.get('system_info', {}).get('timestamp', 'unknown')
        else:
            summary['model_name'] = 'unknown'
            summary['model_path'] = 'unknown'
            summary['timestamp'] = 'unknown'

        # ëª¨ë¸ íƒ€ì… ê²°ì •
        model_name = summary['model_name'].lower()
        if 'engine' in model_name or 'trt' in model_name:
            if 'fp16' in model_name:
                summary['model_type'] = 'TensorRT-FP16'
            elif 'int8' in model_name:
                summary['model_type'] = 'TensorRT-INT8'
            else:
                summary['model_type'] = 'TensorRT'
        elif '.pt' in model_name or 'pytorch' in model_name:
            if 'fp16' in model_name:
                summary['model_type'] = 'PyTorch-FP16'
            elif 'int8' in model_name:
                summary['model_type'] = 'PyTorch-INT8'
            else:
                summary['model_type'] = 'PyTorch'
        else:
            summary['model_type'] = 'Unknown'

        # ëª¨ë¸ í¬ê¸° ì¶”ì •
        if 'yolo11n' in model_name:
            summary['model_size'] = 'nano'
        elif 'yolo11s' in model_name:
            summary['model_size'] = 'small'
        elif 'yolo11m' in model_name:
            summary['model_size'] = 'medium'
        elif 'yolo11l' in model_name:
            summary['model_size'] = 'large'
        else:
            summary['model_size'] = 'unknown'

        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶”ì¶œ
        if 'performance' in data and 'error' not in data['performance']:
            perf = data['performance'].get('performance', data['performance'])
            summary['fps'] = perf.get('fps', 0)
            summary['avg_latency_ms'] = perf.get('avg_latency_ms', 0)
            summary['throughput'] = perf.get('throughput', 0)
            summary['success_rate'] = perf.get('success_rate', 0)
        else:
            summary['fps'] = None
            summary['avg_latency_ms'] = None
            summary['throughput'] = None
            summary['success_rate'] = None

        # ì •í™•ë„ ë©”íŠ¸ë¦­ ì¶”ì¶œ
        if 'accuracy' in data and 'error' not in data['accuracy']:
            acc = data['accuracy']
            summary['map50'] = acc.get('map50', 0)
            summary['map50_95'] = acc.get('map50_95', 0)
            summary['precision'] = acc.get('precision', 0)
            summary['recall'] = acc.get('recall', 0)
            summary['f1_score'] = acc.get('f1_score', 0)
        elif 'results' in data and 'basic_validation' in data['results']:
            # accuracy_benchmark ê²°ê³¼
            acc = data['results']['basic_validation']
            summary['map50'] = acc.get('map50', 0)
            summary['map50_95'] = acc.get('map50_95', 0)
            summary['precision'] = acc.get('precision', 0)
            summary['recall'] = acc.get('recall', 0)
            summary['f1_score'] = acc.get('f1_score', 0)
        else:
            summary['map50'] = None
            summary['map50_95'] = None
            summary['precision'] = None
            summary['recall'] = None
            summary['f1_score'] = None

        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        if summary['fps'] is not None and summary['map50'] is not None:
            # ì„±ëŠ¥ ì ìˆ˜ (FPS ê¸°ì¤€)
            fps_score = min(100, summary['fps'] * 5)  # 20 FPS = 100ì 

            # ì •í™•ë„ ì ìˆ˜ (mAP ê¸°ì¤€)
            acc_score = summary['map50'] * 100

            # ì¢…í•© ì ìˆ˜ (ì„±ëŠ¥ 40% + ì •í™•ë„ 60%)
            summary['overall_score'] = fps_score * 0.4 + acc_score * 0.6
        else:
            summary['overall_score'] = None

        return summary

    def generate_comparison_report(self) -> str:
        """ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±"""
        if self.summary_df is None or self.summary_df.empty:
            return "ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

        df = self.summary_df

        # ë¦¬í¬íŠ¸ ìƒì„±
        report = []
        report.append("# ğŸ† ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ë¶„ì„ ë¦¬í¬íŠ¸")
        report.append(f"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"ë¶„ì„ ëª¨ë¸ ìˆ˜: {len(df)}ê°œ")
        report.append("")

        # 1. ì „ì²´ ìš”ì•½
        report.append("## ğŸ“Š ì „ì²´ ìš”ì•½")
        report.append("| ìˆœìœ„ | ëª¨ë¸ | íƒ€ì… | FPS | mAP@0.5 | ì¢…í•©ì ìˆ˜ | í‰ê°€ |")
        report.append("|------|------|------|-----|---------|----------|------|")

        # ì¢…í•© ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        valid_df = df.dropna(subset=['overall_score']).sort_values('overall_score', ascending=False)

        for idx, (_, row) in enumerate(valid_df.head(10).iterrows(), 1):
            grade = self._get_grade(row['overall_score'])
            report.append(f"| {idx} | {row['model_name']} | {row['model_type']} | "
                         f"{row['fps']:.1f} | {row['map50']:.3f} | {row['overall_score']:.1f} | {grade} |")

        report.append("")

        # 2. ì„±ëŠ¥ TOP 5
        report.append("## âš¡ ì„±ëŠ¥ TOP 5 (FPS)")
        perf_df = df.dropna(subset=['fps']).sort_values('fps', ascending=False)
        for idx, (_, row) in enumerate(perf_df.head(5).iterrows(), 1):
            report.append(f"{idx}. **{row['model_name']}** ({row['model_type']}) - {row['fps']:.1f} FPS")

        report.append("")

        # 3. ì •í™•ë„ TOP 5
        report.append("## ğŸ¯ ì •í™•ë„ TOP 5 (mAP@0.5)")
        acc_df = df.dropna(subset=['map50']).sort_values('map50', ascending=False)
        for idx, (_, row) in enumerate(acc_df.head(5).iterrows(), 1):
            report.append(f"{idx}. **{row['model_name']}** ({row['model_type']}) - {row['map50']:.3f}")

        report.append("")

        # 4. ëª¨ë¸ íƒ€ì…ë³„ ë¶„ì„
        report.append("## ğŸ”„ ëª¨ë¸ íƒ€ì…ë³„ ë¹„êµ")
        type_stats = df.groupby('model_type').agg({
            'fps': ['mean', 'std', 'count'],
            'map50': ['mean', 'std'],
            'overall_score': ['mean', 'std']
        }).round(3)

        for model_type in type_stats.index:
            stats = type_stats.loc[model_type]
            report.append(f"### {model_type}")
            report.append(f"- ëª¨ë¸ ìˆ˜: {stats[('fps', 'count')]}ê°œ")
            if not pd.isna(stats[('fps', 'mean')]):
                report.append(f"- í‰ê·  FPS: {stats[('fps', 'mean')]:.1f} Â± {stats[('fps', 'std')]:.1f}")
            if not pd.isna(stats[('map50', 'mean')]):
                report.append(f"- í‰ê·  mAP@0.5: {stats[('map50', 'mean')]:.3f} Â± {stats[('map50', 'std')]:.3f}")
            if not pd.isna(stats[('overall_score', 'mean')]):
                report.append(f"- í‰ê·  ì¢…í•©ì ìˆ˜: {stats[('overall_score', 'mean')]:.1f}")
            report.append("")

        # 5. ì¶”ì²œì‚¬í•­
        report.append("## ğŸ’¡ ì¶”ì²œì‚¬í•­")

        # ìµœê³  ì„±ëŠ¥
        if not perf_df.empty:
            best_perf = perf_df.iloc[0]
            report.append(f"- **ìµœê³  ì„±ëŠ¥**: {best_perf['model_name']} ({best_perf['fps']:.1f} FPS)")

        # ìµœê³  ì •í™•ë„
        if not acc_df.empty:
            best_acc = acc_df.iloc[0]
            report.append(f"- **ìµœê³  ì •í™•ë„**: {best_acc['model_name']} (mAP@0.5: {best_acc['map50']:.3f})")

        # ê· í˜• ì¡íŒ ëª¨ë¸
        if not valid_df.empty:
            best_overall = valid_df.iloc[0]
            report.append(f"- **ê· í˜• ì¡íŒ ëª¨ë¸**: {best_overall['model_name']} (ì¢…í•©ì ìˆ˜: {best_overall['overall_score']:.1f})")

        return "\n".join(report)

    def _get_grade(self, score: float) -> str:
        """ì ìˆ˜ì— ë”°ë¥¸ ë“±ê¸‰ ë°˜í™˜"""
        if score >= 90:
            return "ğŸ”¥ Sê¸‰"
        elif score >= 80:
            return "ğŸš€ Aê¸‰"
        elif score >= 70:
            return "âœ… Bê¸‰"
        elif score >= 60:
            return "ğŸ“Š Cê¸‰"
        else:
            return "âš ï¸ Dê¸‰"

    def create_visualizations(self, output_dir: str = "./analysis_plots"):
        """ì‹œê°í™” ì°¨íŠ¸ ìƒì„±"""
        if self.summary_df is None or self.summary_df.empty:
            logger.warning("ì‹œê°í™”í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
    
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
    
        df = self.summary_df.dropna(subset=['fps', 'map50'])
    
        if df.empty:
            logger.warning("ì‹œê°í™”í•  ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
    
        # âœ… ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ ì¶”ê°€
        def parse_model_name(model_name):
            """ëª¨ë¸ëª…ì—ì„œ ë² ì´ìŠ¤ëª…ê³¼ ì •ë°€ë„ë¥¼ ë¶„ë¦¬"""
            if '_int8' in model_name or 'int8' in model_name.lower():
                return model_name.replace('_int8', '').replace('-int8', ''), 'int8'
            elif '_fp16' in model_name or 'fp16' in model_name.lower():
                return model_name.replace('_fp16', '').replace('-fp16', ''), 'fp16'
            else:
                return model_name, 'fp32'
    
        # âœ… ë°ì´í„° ì „ì²˜ë¦¬
        df_processed = df.copy()
        df_processed[['base_model', 'precision']] = df_processed['model_name'].apply(
            lambda x: pd.Series(parse_model_name(x))
        )
        
        precision_order = ['int8', 'fp16', 'fp32']
        df_processed['precision'] = pd.Categorical(
            df_processed['precision'], 
            categories=precision_order, 
            ordered=True
        )
    
        # 1. ì„±ëŠ¥ vs ì •í™•ë„ ì‚°ì ë„ (ë¼ì¸ ì—°ê²° ì¶”ê°€)
        plt.figure(figsize=(12, 8))
        scatter = plt.scatter(df['fps'], df['map50'],
                             c=df['overall_score'],
                             s=100,
                             alpha=0.7,
                             cmap='viridis')
    
        # âœ… ê°™ì€ ë² ì´ìŠ¤ ëª¨ë¸ë¼ë¦¬ ë¼ì¸ìœ¼ë¡œ ì—°ê²°
        base_models = df_processed['base_model'].unique()
        colors = plt.cm.Set1(np.linspace(0, 1, len(base_models)))
    
        for i, base_model in enumerate(base_models):
            model_data = df_processed[df_processed['base_model'] == base_model]
            
            if len(model_data) > 1:  # ì—°ê²°í•  ì ì´ 2ê°œ ì´ìƒì¼ ë•Œë§Œ
                # ì •ë°€ë„ ìˆœì„œëŒ€ë¡œ ì •ë ¬
                model_data = model_data.sort_values('precision')
                
                # ë¼ì¸ìœ¼ë¡œ ì—°ê²°
                plt.plot(model_data['fps'], model_data['map50'], 
                        '-', color=colors[i % len(colors)], 
                        alpha=0.5, linewidth=1.5)
    
        for idx, row in df.iterrows():
            plt.annotate(row['model_name'],
                        (row['fps'], row['map50']),
                        xytext=(5, 5),
                        textcoords='offset points',
                        fontsize=8)
    
        plt.colorbar(scatter, label='ì¢…í•© ì ìˆ˜')
        plt.xlabel('FPS (ì„±ëŠ¥)')
        plt.ylabel('mAP@0.5 (ì •í™•ë„)')
        plt.title('ëª¨ë¸ ì„±ëŠ¥ vs ì •í™•ë„ ë¹„êµ (ê°™ì€ ëª¨ë¸ì˜ ì •ë°€ë„ë³„ ì—°ê²°)')
        plt.grid(True, alpha=0.3)
    
        plot_path = output_dir / 'performance_vs_accuracy.png'
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        logger.info(f"ğŸ“ˆ ì°¨íŠ¸ ì €ì¥: {plot_path}")
    
        # 2. ì •ë°€ë„ë³„ ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸
        fig, ax2 = plt.subplots(figsize=(14, 8))
    
        sns.pointplot(data=df_processed, 
                      x='precision', y='map50', hue='base_model',
                      ax=ax2, dodge=0.3, join=True, 
                      markers='o', linestyles='-', linewidth=2, markersize=8)
    
        ax2.set_xlabel('ì •ë°€ë„ íƒ€ì…', fontsize=12)
        ax2.set_ylabel('mAP@0.5', fontsize=12)
        ax2.set_title('ëª¨ë¸ë³„ ì •ë°€ë„ì— ë”°ë¥¸ mAP@0.5 (í‰ê·  Â± 95% ì‹ ë¢°êµ¬ê°„)', fontsize=14)
        ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        ax2.grid(True, alpha=0.3)
    
        plt.tight_layout()
    
        plot_path = output_dir / 'model_precision_seaborn.png'
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        logger.info(f"ğŸ“ˆ Seaborn ì°¨íŠ¸ ì €ì¥: {plot_path}")
    
        # 3. ëª¨ë¸ íƒ€ì…ë³„ ë°•ìŠ¤ í”Œë¡¯
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
        # FPS ë°•ìŠ¤ í”Œë¡¯
        df.boxplot(column='fps', by='model_type', ax=ax1)
        ax1.set_title('ëª¨ë¸ íƒ€ì…ë³„ FPS ë¶„í¬')
        ax1.set_xlabel('ëª¨ë¸ íƒ€ì…')
        ax1.set_ylabel('FPS')
    
        # mAP ë°•ìŠ¤ í”Œë¡¯
        df.boxplot(column='map50', by='model_type', ax=ax2)
        ax2.set_title('ëª¨ë¸ íƒ€ì…ë³„ mAP@0.5 ë¶„í¬')
        ax2.set_xlabel('ëª¨ë¸ íƒ€ì…')
        ax2.set_ylabel('mAP@0.5')
    
        plt.suptitle('')  # ìë™ ì œëª© ì œê±°
    
        plot_path = output_dir / 'model_type_comparison.png'
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        logger.info(f"ğŸ“ˆ ì°¨íŠ¸ ì €ì¥: {plot_path}")
    
        # 4. ì¢…í•© ì ìˆ˜ íˆíŠ¸ë§µ
        pivot_df = df.pivot_table(
            values='overall_score',
            index='model_size',
            columns='model_type',
            aggfunc='mean'
        )
    
        if not pivot_df.empty:
            plt.figure(figsize=(10, 6))
            sns.heatmap(pivot_df, annot=True, fmt='.1f', cmap='YlOrRd')
            plt.title('ëª¨ë¸ í¬ê¸° Ã— íƒ€ì…ë³„ ì¢…í•© ì ìˆ˜')
    
            plot_path = output_dir / 'score_heatmap.png'
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            plt.close()
            logger.info(f"ğŸ“ˆ ì°¨íŠ¸ ì €ì¥: {plot_path}")
    
    def export_csv(self, output_path: str = "./benchmark_summary.csv"):
        """ê²°ê³¼ë¥¼ CSVë¡œ ë‚´ë³´ë‚´ê¸°"""
        if self.summary_df is None:
            logger.warning("ë‚´ë³´ë‚¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        self.summary_df.to_csv(output_path, index=False, encoding='utf-8-sig')
        logger.info(f"ğŸ’¾ CSV ì €ì¥: {output_path}")

    def print_summary(self):
        """ìš”ì•½ ì •ë³´ ì½˜ì†” ì¶œë ¥"""
        if self.summary_df is None or self.summary_df.empty:
            print("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        df = self.summary_df

        print("\n" + "="*80)
        print("ğŸ† ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì¢…í•© ë¶„ì„")
        print("="*80)

        # TOP 3 ì¶œë ¥
        valid_df = df.dropna(subset=['overall_score']).sort_values('overall_score', ascending=False)

        if not valid_df.empty:
            print("\nğŸ¥‡ TOP 3 ëª¨ë¸ (ì¢…í•© ì ìˆ˜)")
            for idx, (_, row) in enumerate(valid_df.head(3).iterrows(), 1):
                grade = self._get_grade(row['overall_score'])
                print(f"{idx}. {row['model_name']} ({row['model_type']})")
                print(f"   ì¢…í•©ì ìˆ˜: {row['overall_score']:.1f} {grade}")
                print(f"   FPS: {row['fps']:.1f}, mAP@0.5: {row['map50']:.3f}")
                print()

        print("="*80)


def main():
    parser = argparse.ArgumentParser(description="ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì¢…í•© ë¶„ì„")
    parser.add_argument('results_dir', help='ê²°ê³¼ íŒŒì¼ ë””ë ‰í† ë¦¬')
    parser.add_argument('--pattern', default='*.json', help='íŒŒì¼ íŒ¨í„´ (ê¸°ë³¸ê°’: *.json)')
    parser.add_argument('--export-csv', action='store_true', help='CSVë¡œ ë‚´ë³´ë‚´ê¸°')
    parser.add_argument('--save-report', action='store_true', help='ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ì €ì¥')
    parser.add_argument('--plot', action='store_true', help='ì‹œê°í™” ì°¨íŠ¸ ìƒì„±')
    parser.add_argument('--output-dir', default='./analysis_output', help='ì¶œë ¥ ë””ë ‰í† ë¦¬')

    args = parser.parse_args()

    if not Path(args.results_dir).exists():
        logger.error(f"âŒ ê²°ê³¼ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.results_dir}")
        sys.exit(1)

    try:
        # ë¶„ì„ê¸° ì´ˆê¸°í™”
        analyzer = ResultAnalyzer(args.results_dir)

        # ê²°ê³¼ ë¡œë“œ
        loaded_count = analyzer.load_all_results(args.pattern)
        if loaded_count == 0:
            logger.error("âŒ ë¡œë“œëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)

        # ìš”ì•½ ë°ì´í„° ì¶”ì¶œ
        summary_df = analyzer.extract_summary_data()
        if summary_df.empty:
            logger.error("âŒ ë¶„ì„ ê°€ëŠ¥í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)

        # ì½˜ì†” ìš”ì•½ ì¶œë ¥
        analyzer.print_summary()

        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir = Path(args.output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        # CSV ë‚´ë³´ë‚´ê¸°
        if args.export_csv:
            csv_path = output_dir / f"benchmark_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
            analyzer.export_csv(csv_path)

        # ë¦¬í¬íŠ¸ ì €ì¥
        if args.save_report:
            report = analyzer.generate_comparison_report()
            report_path = output_dir / f"benchmark_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report)
            logger.info(f"ğŸ“ ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")

        # ì‹œê°í™”
        if args.plot:
            analyzer.create_visualizations(output_dir / 'plots')

        logger.info("ğŸ‰ ë¶„ì„ ì™„ë£Œ!")

    except KeyboardInterrupt:
        logger.info("âŒ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()