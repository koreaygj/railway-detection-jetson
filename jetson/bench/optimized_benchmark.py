#!/usr/bin/env python3
"""
ìµœì í™”ëœ ë‹¨ì¼ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸
YOLO predict ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ë°°í¬ í™˜ê²½ê³¼ ë™ì¼í•œ ì„±ëŠ¥ ì¸¡ì •

Author: Claude Code
"""

import os
import sys
import time
import json
import yaml
import logging
import argparse
import platform
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional, Union

import numpy as np
import cv2
from tqdm import tqdm
from ultralytics import YOLO

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class OptimizedBenchmark:
    def __init__(self, dataset_path: Union[str, Path], model_path: Union[str, Path]):
        """
        ìµœì í™”ëœ ë²¤ì¹˜ë§ˆí¬ ì´ˆê¸°í™”

        Args:
            dataset_path: ë°ì´í„°ì…‹ ê²½ë¡œ (YAML íŒŒì¼ ë˜ëŠ” ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬)
            model_path: ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (.engine, .pt, .onnx)
        """
        self.dataset_path = Path(dataset_path)
        self.model_path = Path(model_path)

        # ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘
        self.system_info = self._collect_system_info()

        # ë°ì´í„°ì…‹ ë¡œë“œ
        self.image_paths = self._load_dataset()

        # ëª¨ë¸ ì •ë³´
        self.model_type = self._determine_model_type()
        self.model_info = self._collect_model_info()

        logger.info(f"ğŸš€ ìµœì í™”ëœ ë²¤ì¹˜ë§ˆí¬ ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"   ëª¨ë¸: {self.model_path.name} ({self.model_type})")
        logger.info(f"   ë°ì´í„°ì…‹: {len(self.image_paths)}ê°œ ì´ë¯¸ì§€")
        logger.info(f"   í”Œë«í¼: {self.system_info['platform']}")

    def _collect_system_info(self) -> Dict:
        """ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘"""
        import psutil

        system_info = {
            'platform': platform.system(),
            'platform_release': platform.release(),
            'platform_version': platform.version(),
            'architecture': platform.machine(),
            'hostname': platform.node(),
            'processor': platform.processor(),
            'python_version': platform.python_version(),
            'cpu_count': psutil.cpu_count(),
            'memory_total': psutil.virtual_memory().total,
            'timestamp': datetime.now().isoformat()
        }

        # GPU ì •ë³´ (ê°€ëŠ¥í•œ ê²½ìš°)
        try:
            import pynvml
            pynvml.nvmlInit()
            gpu_count = pynvml.nvmlDeviceGetCount()
            system_info['gpu_count'] = gpu_count

            gpu_info = []
            for i in range(gpu_count):
                handle = pynvml.nvmlDeviceGetHandleByIndex(i)
                name = pynvml.nvmlDeviceGetName(handle).decode('utf-8')
                memory = pynvml.nvmlDeviceGetMemoryInfo(handle)
                gpu_info.append({
                    'name': name,
                    'memory_total': memory.total,
                    'memory_free': memory.free
                })
            system_info['gpu_info'] = gpu_info
        except ImportError:
            system_info['gpu_info'] = "pynvml not available"
        except Exception as e:
            system_info['gpu_info'] = f"Error: {str(e)}"

        return system_info

    def _load_dataset(self) -> List[Path]:
        """ë°ì´í„°ì…‹ì—ì„œ ì´ë¯¸ì§€ ê²½ë¡œ ë¡œë“œ (YOLO í˜¸í™˜ ë°©ì‹)"""
        image_paths = []

        if self.dataset_path.suffix.lower() == '.yaml':
            # YAML ë°ì´í„°ì…‹ - YOLO í˜¸í™˜ ê²€ì¦ ë¨¼ì € ìˆ˜í–‰
            logger.info(f"YAML ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘: {self.dataset_path}")

            # YOLO í˜¸í™˜ ê²½ë¡œ í•´ì„ (accuracy_benchmarkì™€ ë™ì¼ ë°©ì‹)
            logger.info("YOLO í˜¸í™˜ ê²½ë¡œ í•´ì„ ì¤‘...")

            # YAML íŒŒì¼ì—ì„œ ê²½ë¡œ ì •ë³´ ì¶”ì¶œ
            with open(self.dataset_path, 'r', encoding='utf-8') as f:
                data_config = yaml.safe_load(f)

            # ë°ì´í„°ì…‹ ê²½ë¡œ í•´ì„ (YOLO ë°©ì‹)
            dataset_root = Path(data_config.get('path', '.'))
            if not dataset_root.is_absolute():
                # YAML íŒŒì¼ ê¸°ì¤€ìœ¼ë¡œ ìƒëŒ€ ê²½ë¡œ í•´ì„
                dataset_root = self.dataset_path.parent / dataset_root

            # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
            dataset_root = dataset_root.resolve()

            # ê²€ì¦ ì´ë¯¸ì§€ ê²½ë¡œ
            val_path = data_config.get('val', 'val/images')
            val_images_dir = dataset_root / val_path

            logger.info(f"ë°ì´í„°ì…‹ ë£¨íŠ¸: {dataset_root}")
            logger.info(f"ê²€ì¦ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬: {val_images_dir}")

            # ê²½ë¡œ ì¡´ì¬ í™•ì¸
            if not val_images_dir.exists():
                # ë‹¤ë¥¸ ê°€ëŠ¥í•œ ê²½ë¡œë“¤ ì‹œë„
                possible_paths = [
                    dataset_root / 'val' / 'images',
                    dataset_root / 'valid' / 'images',
                    dataset_root / 'validation' / 'images',
                    dataset_root / 'test' / 'images',
                    self.dataset_path.parent / 'val' / 'images',
                    self.dataset_path.parent / 'data' / 'val' / 'images'
                ]

                for possible_path in possible_paths:
                    if possible_path.exists():
                        logger.info(f"âœ… ëŒ€ì²´ ê²€ì¦ ê²½ë¡œ ë°œê²¬: {possible_path}")
                        val_images_dir = possible_path
                        break
                else:
                    raise ValueError(f"ê²€ì¦ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {val_images_dir}")

            # ì´ë¯¸ì§€ íŒŒì¼ ì°¾ê¸°
            image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp']
            for ext in image_extensions:
                image_paths.extend(val_images_dir.rglob(f"*{ext}"))
                image_paths.extend(val_images_dir.rglob(f"*{ext.upper()}"))

        elif self.dataset_path.is_dir():
            # ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬
            logger.info(f"ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ë¡œë“œ ì¤‘: {self.dataset_path}")

            image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp']
            for ext in image_extensions:
                image_paths.extend(self.dataset_path.rglob(f"*{ext}"))
                image_paths.extend(self.dataset_path.rglob(f"*{ext.upper()}"))
        else:
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ë°ì´í„°ì…‹ í˜•ì‹: {self.dataset_path}")

        if not image_paths:
            raise ValueError(f"ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.dataset_path}")

        logger.info(f"ğŸ“ ì´ {len(image_paths)}ê°œ ì´ë¯¸ì§€ ë°œê²¬")
        return sorted(image_paths)

    def _determine_model_type(self) -> str:
        """ëª¨ë¸ íƒ€ì… ê²°ì •"""
        suffix = self.model_path.suffix.lower()

        model_types = {
            '.engine': 'TensorRT',
            '.pt': 'PyTorch',
            '.pth': 'PyTorch',
            '.onnx': 'ONNX'
        }

        return model_types.get(suffix, 'Unknown')

    def _collect_model_info(self) -> Dict:
        """ëª¨ë¸ ì •ë³´ ìˆ˜ì§‘"""
        model_info = {
            'file_path': str(self.model_path),
            'file_name': self.model_path.name,
            'file_size': self.model_path.stat().st_size,
            'model_type': self.model_type,
            'created_time': datetime.fromtimestamp(self.model_path.stat().st_ctime).isoformat(),
            'modified_time': datetime.fromtimestamp(self.model_path.stat().st_mtime).isoformat()
        }

        return model_info

    def run_benchmark(self,
                     max_images: Optional[int] = None,
                     warmup_runs: int = 5,
                     device: str = '0',
                     conf_threshold: float = 0.25,
                     iou_threshold: float = 0.45,
                     imgsz: int = 640) -> Dict:
        """
        ìµœì í™”ëœ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰

        Args:
            max_images: ìµœëŒ€ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìˆ˜
            warmup_runs: ì›Œë°ì—… ì‹¤í–‰ íšŸìˆ˜
            device: ì‹¤í–‰ ë””ë°”ì´ìŠ¤
            conf_threshold: ì‹ ë¢°ë„ ì„ê³„ê°’
            iou_threshold: NMS IOU ì„ê³„ê°’
            imgsz: ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°

        Returns:
            Dict: ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
        """
        logger.info(f"ğŸƒ ìµœì í™”ëœ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
        logger.info(f"   ëª¨ë¸: {self.model_path.name}")
        logger.info(f"   ë””ë°”ì´ìŠ¤: {device}")
        logger.info(f"   ì´ë¯¸ì§€ í¬ê¸°: {imgsz}")
        logger.info(f"   ì›Œë°ì—…: {warmup_runs}íšŒ")

        # ëª¨ë¸ ë¡œë“œ
        logger.info("ğŸ“¦ ëª¨ë¸ ë¡œë“œ ì¤‘...")
        model_load_start = time.time()

        try:
            model = YOLO(str(self.model_path))
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

        model_load_time = time.time() - model_load_start
        logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({model_load_time:.2f}s)")

        # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ì„ íƒ
        test_images = self.image_paths[:max_images] if max_images else self.image_paths
        logger.info(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€: {len(test_images)}ê°œ")

        # ì›Œë°ì—…
        logger.info(f"ğŸ”¥ ì›Œë°ì—… ì‹¤í–‰ ì¤‘... ({warmup_runs}íšŒ)")
        dummy_img = np.random.randint(0, 255, (imgsz, imgsz, 3), dtype=np.uint8)

        warmup_start = time.time()
        for i in range(warmup_runs):
            try:
                _ = model.predict(
                    dummy_img,
                    device=device,
                    verbose=False,
                    save=False,
                    conf=conf_threshold,
                    iou=iou_threshold,
                    imgsz=imgsz
                )
                if (i + 1) % 2 == 0:
                    logger.info(f"  ì›Œë°ì—… ì§„í–‰: {i + 1}/{warmup_runs}")
            except Exception as e:
                logger.warning(f"ì›Œë°ì—… ì¤‘ ì˜¤ë¥˜: {e}")

        warmup_time = time.time() - warmup_start
        logger.info(f"âœ… ì›Œë°ì—… ì™„ë£Œ ({warmup_time:.2f}s)")

        # ë©”ì¸ ë²¤ì¹˜ë§ˆí¬
        logger.info("âš¡ ë©”ì¸ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘...")

        # ì¸¡ì • ë³€ìˆ˜
        inference_times = []
        preprocessing_times = []
        postprocessing_times = []
        total_times = []
        detection_counts = []
        successful_inferences = 0
        failed_inferences = 0

        benchmark_start = time.time()

        # ì´ë¯¸ì§€ë³„ ì¶”ë¡ 
        for i, img_path in enumerate(tqdm(test_images, desc="Processing")):
            try:
                # ì´ë¯¸ì§€ ë¡œë“œ
                img_load_start = time.time()
                image = cv2.imread(str(img_path))
                if image is None:
                    failed_inferences += 1
                    continue
                img_load_time = time.time() - img_load_start

                # ì¶”ë¡  ì‹¤í–‰ (TensorRT ì•ˆì „ ëª¨ë“œ)
                total_start = time.time()

                try:
                    # TensorRT ì—”ì§„ì¸ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
                    if str(self.model_path).endswith('.engine'):
                        # TensorRT ì—”ì§„ì— ëŒ€í•´ ë” ê´€ëŒ€í•œ ì„¤ì •
                        results = model.predict(
                            image,
                            device=device,
                            verbose=False,
                            save=False,
                            conf=conf_threshold,
                            iou=iou_threshold,
                            imgsz=imgsz,
                            half=False,  # TensorRTëŠ” ì´ë¯¸ ìµœì í™”ë¨
                            augment=False,  # ë°ì´í„° ì¦ê°• ë¹„í™œì„±í™”
                            agnostic_nms=False,  # í´ë˜ìŠ¤ë³„ NMS
                            max_det=300  # ìµœëŒ€ ê²€ì¶œ ìˆ˜ ì œí•œ
                        )
                    else:
                        # PyTorch ëª¨ë¸ ê¸°ë³¸ ì„¤ì •
                        results = model.predict(
                            image,
                            device=device,
                            verbose=False,
                            save=False,
                            conf=conf_threshold,
                            iou=iou_threshold,
                            imgsz=imgsz
                        )

                    total_time = time.time() - total_start

                    # ê²°ê³¼ ì²˜ë¦¬ (ì•ˆì „í•œ ë°©ì‹)
                    num_detections = 0
                    if results and len(results) > 0:
                        result = results[0]

                        # ì—¬ëŸ¬ ë°©ì‹ìœ¼ë¡œ ê²€ì¶œ ìˆ˜ í™•ì¸
                        if hasattr(result, 'boxes') and result.boxes is not None:
                            try:
                                num_detections = len(result.boxes)
                            except:
                                num_detections = 0
                        else:
                            # ê²°ê³¼ê°€ ìˆë‹¤ê³  ê°€ì •í•˜ê³  1ê°œë¡œ ê³„ì‚°
                            num_detections = 1 if results else 0

                except Exception as predict_error:
                    # predict ë©”ì†Œë“œ ìì²´ì—ì„œ ì˜¤ë¥˜ ë°œìƒ
                    total_time = time.time() - total_start
                    num_detections = 0

                    # ì˜¤ë¥˜ íƒ€ì…ì— ë”°ë¼ ë‹¤ë¥¸ ì²˜ë¦¬
                    if "'images'" in str(predict_error):
                        logger.warning(f"TensorRT ì¶œë ¥ êµ¬ì¡° ë¬¸ì œë¡œ ê±´ë„ˆë›°ê¸°: {img_path.name}")
                        failed_inferences += 1
                        continue
                    else:
                        logger.warning(f"ì¶”ë¡  ì˜¤ë¥˜: {predict_error}")
                        failed_inferences += 1
                        continue

                # ì‹œê°„ ê¸°ë¡
                total_times.append(total_time)
                detection_counts.append(num_detections)
                successful_inferences += 1

                # ì§„í–‰ ìƒí™© ì¶œë ¥ (ë§¤ 20ê°œë§ˆë‹¤)
                if (i + 1) % 20 == 0:
                    recent_fps = 1.0 / np.mean(total_times[-20:])
                    logger.info(f"  ì§„í–‰: {i+1}/{len(test_images)} - í˜„ì¬ FPS: {recent_fps:.1f}")

            except Exception as outer_e:
                # ìµœì™¸ê³½ ì˜ˆì™¸ ì²˜ë¦¬ (ì´ë¯¸ì§€ ë¡œë“œ ë“±)
                failed_inferences += 1
                logger.warning(f"ì´ë¯¸ì§€ {img_path.name} ì™¸ë¶€ ì²˜ë¦¬ ì‹¤íŒ¨: {outer_e}")
                continue

        benchmark_time = time.time() - benchmark_start

        # ê²°ê³¼ ê³„ì‚°
        if not total_times:
            logger.error("âŒ ì„±ê³µí•œ ì¶”ë¡ ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None

        # í†µê³„ ê³„ì‚°
        total_times_np = np.array(total_times)
        detection_counts_np = np.array(detection_counts)

        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        performance_metrics = {
            'total_images_processed': successful_inferences,
            'failed_inferences': failed_inferences,
            'success_rate': successful_inferences / len(test_images),

            # ì§€ì—°ì‹œê°„ í†µê³„ (ms)
            'avg_latency_ms': float(np.mean(total_times_np) * 1000),
            'std_latency_ms': float(np.std(total_times_np) * 1000),
            'min_latency_ms': float(np.min(total_times_np) * 1000),
            'max_latency_ms': float(np.max(total_times_np) * 1000),
            'p50_latency_ms': float(np.percentile(total_times_np, 50) * 1000),
            'p95_latency_ms': float(np.percentile(total_times_np, 95) * 1000),
            'p99_latency_ms': float(np.percentile(total_times_np, 99) * 1000),

            # FPS í†µê³„
            'fps': float(1.0 / np.mean(total_times_np)),
            'min_fps': float(1.0 / np.max(total_times_np)),
            'max_fps': float(1.0 / np.min(total_times_np)),
            'throughput': float(successful_inferences / benchmark_time),

            # ê²€ì¶œ í†µê³„
            'avg_detections_per_image': float(np.mean(detection_counts_np)),
            'total_detections': int(np.sum(detection_counts_np)),
            'max_detections_per_image': int(np.max(detection_counts_np)),
        }

        # ì „ì²´ ê²°ê³¼ êµ¬ì„±
        results = {
            'model_info': self.model_info,
            'performance': performance_metrics,
            'benchmark_config': {
                'max_images': max_images,
                'warmup_runs': warmup_runs,
                'device': device,
                'conf_threshold': conf_threshold,
                'iou_threshold': iou_threshold,
                'imgsz': imgsz,
                'model_load_time': model_load_time,
                'warmup_time': warmup_time,
                'total_benchmark_time': benchmark_time,
                'timestamp': datetime.now().isoformat()
            },
            'dataset_info': {
                'dataset_path': str(self.dataset_path),
                'total_images_available': len(self.image_paths),
                'images_tested': len(test_images)
            },
            'system_info': self.system_info
        }

        # ê²°ê³¼ ì¶œë ¥
        self._print_summary(results)

        return results

    def _print_summary(self, results: Dict):
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        perf = results['performance']
        config = results['benchmark_config']

        print("\n" + "="*60)
        print("ğŸ“Š ìµœì í™”ëœ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼")
        print("="*60)
        print(f"ğŸ¤– ëª¨ë¸: {results['model_info']['file_name']}")
        print(f"ğŸ¯ ëª¨ë¸ íƒ€ì…: {results['model_info']['model_type']}")
        print(f"ğŸ“ ì´ë¯¸ì§€ í¬ê¸°: {config['imgsz']}x{config['imgsz']}")
        print(f"ğŸ–¥ï¸  ë””ë°”ì´ìŠ¤: {config['device']}")
        print("")

        print("âš¡ ì„±ëŠ¥ ë©”íŠ¸ë¦­:")
        print(f"   í‰ê·  FPS:           {perf['fps']:.2f}")
        print(f"   ì²˜ë¦¬ëŸ‰:             {perf['throughput']:.2f} images/sec")
        print(f"   í‰ê·  ì§€ì—°ì‹œê°„:       {perf['avg_latency_ms']:.2f} Â± {perf['std_latency_ms']:.2f} ms")
        print(f"   P95 ì§€ì—°ì‹œê°„:        {perf['p95_latency_ms']:.2f} ms")
        print(f"   P99 ì§€ì—°ì‹œê°„:        {perf['p99_latency_ms']:.2f} ms")
        print("")

        print("ğŸ“Š ì²˜ë¦¬ í†µê³„:")
        print(f"   ì„±ê³µí•œ ì¶”ë¡ :         {perf['total_images_processed']}")
        print(f"   ì‹¤íŒ¨í•œ ì¶”ë¡ :         {perf['failed_inferences']}")
        print(f"   ì„±ê³µë¥ :             {perf['success_rate']*100:.1f}%")
        print(f"   í‰ê·  ê²€ì¶œ ìˆ˜:        {perf['avg_detections_per_image']:.2f}")
        print(f"   ì´ ê²€ì¶œ ìˆ˜:          {perf['total_detections']}")
        print("")

        print("â±ï¸  ì‹œê°„ ì •ë³´:")
        print(f"   ëª¨ë¸ ë¡œë“œ:          {config['model_load_time']:.2f}s")
        print(f"   ì›Œë°ì—…:             {config['warmup_time']:.2f}s")
        print(f"   ì´ ë²¤ì¹˜ë§ˆí¬:         {config['total_benchmark_time']:.2f}s")
        print("="*60)

    def save_results(self, results: Dict, output_dir: str = './benchmark_results', custom_name: Optional[str] = None):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        # íŒŒì¼ëª… ìƒì„±
        if custom_name:
            filename = f"optimized_benchmark_{custom_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        else:
            model_name = Path(results['model_info']['file_name']).stem
            model_name_clean = model_name.replace('.', '_')
            filename = f"optimized_benchmark_{model_name_clean}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        output_path = output_dir / filename

        # JSON ì €ì¥
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)

        logger.info(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")
        return output_path


def main():
    parser = argparse.ArgumentParser(description="ìµœì í™”ëœ ë‹¨ì¼ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬")
    parser.add_argument('dataset', help='ë°ì´í„°ì…‹ ê²½ë¡œ (YAML íŒŒì¼ ë˜ëŠ” ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬)')
    parser.add_argument('model', help='ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (.engine, .pt, .onnx)')
    parser.add_argument('--max-images', type=int, help='ìµœëŒ€ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìˆ˜')
    parser.add_argument('--warmup', type=int, default=5, help='ì›Œë°ì—… ì‹¤í–‰ íšŸìˆ˜ (ê¸°ë³¸ê°’: 5)')
    parser.add_argument('--device', default='0', help='ì‹¤í–‰ ë””ë°”ì´ìŠ¤ (ê¸°ë³¸ê°’: 0)')
    parser.add_argument('--conf', type=float, default=0.25, help='ì‹ ë¢°ë„ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.25)')
    parser.add_argument('--iou', type=float, default=0.45, help='NMS IOU ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.45)')
    parser.add_argument('--imgsz', type=int, default=640, help='ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸° (ê¸°ë³¸ê°’: 640)')
    parser.add_argument('--output-dir', default='./benchmark_results', help='ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--name', help='ê²°ê³¼ íŒŒì¼ ì»¤ìŠ¤í…€ ì´ë¦„')
    parser.add_argument('--no-save', action='store_true', help='ê²°ê³¼ ì €ì¥ ì•ˆí•¨')

    args = parser.parse_args()

    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not Path(args.dataset).exists():
        logger.error(f"âŒ ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.dataset}")
        sys.exit(1)

    if not Path(args.model).exists():
        logger.error(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.model}")
        sys.exit(1)

    try:
        # ë²¤ì¹˜ë§ˆí¬ ì´ˆê¸°í™”
        benchmark = OptimizedBenchmark(args.dataset, args.model)

        # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        results = benchmark.run_benchmark(
            max_images=args.max_images,
            warmup_runs=args.warmup,
            device=args.device,
            conf_threshold=args.conf,
            iou_threshold=args.iou,
            imgsz=args.imgsz
        )

        if results is None:
            logger.error("âŒ ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨")
            sys.exit(1)

        # ê²°ê³¼ ì €ì¥
        if not args.no_save:
            benchmark.save_results(results, args.output_dir, args.name)

        logger.info("ğŸ‰ ìµœì í™”ëœ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")

    except KeyboardInterrupt:
        logger.info("âŒ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()