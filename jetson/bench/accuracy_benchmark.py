#!/usr/bin/env python3
"""
ì •í™•ë„ ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸
mAP, ì •ë°€ë„, ì¬í˜„ìœ¨ ë“± ì •í™•ë„ ë©”íŠ¸ë¦­ ì¸¡ì •

ì‚¬ìš©ë²•:
    python accuracy_benchmark.py data.yaml model.engine --save-results
"""

import os
import sys
import time
import json
import yaml
import logging
import argparse
import platform
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional, Union

import numpy as np
import cv2
from tqdm import tqdm
from ultralytics import YOLO

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class AccuracyBenchmark:
    def __init__(self, data_yaml: Union[str, Path], model_path: Union[str, Path]):
        """
        ì •í™•ë„ ë²¤ì¹˜ë§ˆí¬ ì´ˆê¸°í™”

        Args:
            data_yaml: ë°ì´í„°ì…‹ YAML íŒŒì¼ ê²½ë¡œ
            model_path: ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (.engine, .pt, .onnx)
        """
        self.data_yaml = Path(data_yaml)
        self.model_path = Path(model_path)

        # ëª¨ë¸ ì •ë³´
        self.model_type = self._determine_model_type()

        logger.info(f"ğŸ¯ ì •í™•ë„ ë²¤ì¹˜ë§ˆí¬ ì´ˆê¸°í™”")
        logger.info(f"   ë°ì´í„°ì…‹: {self.data_yaml}")
        logger.info(f"   ëª¨ë¸: {self.model_path.name} ({self.model_type})")

    def _determine_model_type(self) -> str:
        """ëª¨ë¸ íƒ€ì… ê²°ì •"""
        suffix = self.model_path.suffix.lower()
        model_types = {
            '.engine': 'TensorRT',
            '.pt': 'PyTorch',
            '.pth': 'PyTorch',
            '.onnx': 'ONNX'
        }
        return model_types.get(suffix, 'Unknown')

    def run_validation(self,
                      device: str = '0',
                      conf_threshold: float = 0.001,  # ë‚®ì€ ì„ê³„ê°’ìœ¼ë¡œ ëª¨ë“  ê²€ì¶œ í¬í•¨
                      iou_threshold: float = 0.6,     # NMS IOU ì„ê³„ê°’
                      max_det: int = 300,             # ìµœëŒ€ ê²€ì¶œ ìˆ˜
                      imgsz: int = 640,
                      save_dir: Optional[str] = None,
                      plots: bool = True,
                      verbose: bool = True) -> Dict:
        """
        YOLO ê²€ì¦ ì‹¤í–‰ (mAP ê³„ì‚°)

        Args:
            device: GPU ë””ë°”ì´ìŠ¤
            conf_threshold: ì‹ ë¢°ë„ ì„ê³„ê°’
            iou_threshold: NMS IOU ì„ê³„ê°’
            max_det: ìµœëŒ€ ê²€ì¶œ ìˆ˜
            imgsz: ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°
            save_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
            plots: ì‹œê°í™” ì°¨íŠ¸ ìƒì„±
            verbose: ìƒì„¸ ë¡œê·¸

        Returns:
            Dict: ê²€ì¦ ê²°ê³¼
        """
        logger.info(f"ğŸ” YOLO ê²€ì¦ ì‹¤í–‰ ì¤‘...")
        logger.info(f"   ì‹ ë¢°ë„ ì„ê³„ê°’: {conf_threshold}")
        logger.info(f"   NMS IOU: {iou_threshold}")
        logger.info(f"   ìµœëŒ€ ê²€ì¶œ: {max_det}")

        try:
            # ëª¨ë¸ ë¡œë“œ
            model = YOLO(str(self.model_path))

            # ê²€ì¦ ì‹¤í–‰
            validation_start = time.time()

            results = model.val(
                data=str(self.data_yaml),
                device=device,
                conf=conf_threshold,
                iou=iou_threshold,
                max_det=max_det,
                imgsz=imgsz,
                save_json=True,
                plots=plots,
                verbose=verbose,
                project=save_dir if save_dir else None,
                name='validation_results'
            )

            validation_time = time.time() - validation_start

            # ê²°ê³¼ ì¶”ì¶œ
            metrics = self._extract_validation_metrics(results, validation_time)

            logger.info("âœ… ê²€ì¦ ì™„ë£Œ")
            self._print_accuracy_summary(metrics)

            return metrics

        except Exception as e:
            logger.error(f"âŒ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return None

    def _extract_validation_metrics(self, results, validation_time: float) -> Dict:
        """ê²€ì¦ ê²°ê³¼ì—ì„œ ë©”íŠ¸ë¦­ ì¶”ì¶œ"""

        # ë©”ì¸ ë©”íŠ¸ë¦­
        metrics = {
            'validation_time': validation_time,
            'map50': float(results.box.map50) if hasattr(results.box, 'map50') else 0.0,
            'map50_95': float(results.box.map) if hasattr(results.box, 'map') else 0.0,
            'precision': float(results.box.mp) if hasattr(results.box, 'mp') else 0.0,
            'recall': float(results.box.mr) if hasattr(results.box, 'mr') else 0.0,
            'f1_score': 0.0,  # ê³„ì‚°ë¨
        }

        # F1 ìŠ¤ì½”ì–´ ê³„ì‚°
        if metrics['precision'] > 0 and metrics['recall'] > 0:
            metrics['f1_score'] = 2 * (metrics['precision'] * metrics['recall']) / (metrics['precision'] + metrics['recall'])

        # í´ë˜ìŠ¤ë³„ mAP (ê°€ëŠ¥í•œ ê²½ìš°)
        try:
            if hasattr(results.box, 'maps') and results.box.maps is not None:
                class_maps = results.box.maps.tolist() if hasattr(results.box.maps, 'tolist') else results.box.maps
                metrics['class_maps'] = class_maps

                # í´ë˜ìŠ¤ ì´ë¦„ê³¼ ë§¤í•‘
                if hasattr(results, 'names') and results.names:
                    class_names = results.names
                    metrics['class_metrics'] = {}
                    for i, (class_id, class_name) in enumerate(class_names.items()):
                        if i < len(class_maps):
                            metrics['class_metrics'][class_name] = {
                                'map50': float(class_maps[i]),
                                'class_id': int(class_id)
                            }
        except Exception as e:
            logger.warning(f"í´ë˜ìŠ¤ë³„ ë©”íŠ¸ë¦­ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

        # ì¶”ê°€ í†µê³„
        try:
            if hasattr(results.box, 'mp') and hasattr(results.box.mp, '__len__'):
                # í´ë˜ìŠ¤ë³„ ì •ë°€ë„/ì¬í˜„ìœ¨
                precisions = results.box.mp.tolist() if hasattr(results.box.mp, 'tolist') else [results.box.mp]
                recalls = results.box.mr.tolist() if hasattr(results.box.mr, 'tolist') else [results.box.mr]

                metrics['per_class_precision'] = precisions
                metrics['per_class_recall'] = recalls
        except Exception as e:
            logger.warning(f"í´ë˜ìŠ¤ë³„ ì •ë°€ë„/ì¬í˜„ìœ¨ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

        return metrics

    def run_speed_vs_accuracy_test(self,
                                  conf_thresholds: List[float] = [0.001, 0.01, 0.1, 0.25, 0.5],
                                  device: str = '0',
                                  max_images: int = 100) -> Dict:
        """
        ë‹¤ì–‘í•œ ì‹ ë¢°ë„ ì„ê³„ê°’ì—ì„œ ì†ë„ vs ì •í™•ë„ í…ŒìŠ¤íŠ¸

        Args:
            conf_thresholds: í…ŒìŠ¤íŠ¸í•  ì‹ ë¢°ë„ ì„ê³„ê°’ë“¤
            device: GPU ë””ë°”ì´ìŠ¤
            max_images: í…ŒìŠ¤íŠ¸í•  ìµœëŒ€ ì´ë¯¸ì§€ ìˆ˜

        Returns:
            Dict: ì†ë„-ì •í™•ë„ íŠ¸ë ˆì´ë“œì˜¤í”„ ê²°ê³¼
        """
        logger.info(f"âš–ï¸  ì†ë„ vs ì •í™•ë„ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        logger.info(f"   ì‹ ë¢°ë„ ì„ê³„ê°’: {conf_thresholds}")

        results = {}

        for conf in conf_thresholds:
            logger.info(f"ğŸ” ì‹ ë¢°ë„ ì„ê³„ê°’ {conf} í…ŒìŠ¤íŠ¸ ì¤‘...")

            try:
                # ì •í™•ë„ ì¸¡ì •
                accuracy_metrics = self.run_validation(
                    device=device,
                    conf_threshold=conf,
                    plots=False,
                    verbose=False
                )

                if accuracy_metrics:
                    results[f'conf_{conf}'] = {
                        'confidence_threshold': conf,
                        'map50': accuracy_metrics['map50'],
                        'map50_95': accuracy_metrics['map50_95'],
                        'precision': accuracy_metrics['precision'],
                        'recall': accuracy_metrics['recall'],
                        'f1_score': accuracy_metrics['f1_score'],
                        'validation_time': accuracy_metrics['validation_time']
                    }

            except Exception as e:
                logger.warning(f"ì‹ ë¢°ë„ {conf}ì—ì„œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
                continue

        return results

    def compare_models(self,
                      other_model_paths: List[Union[str, Path]],
                      device: str = '0') -> Dict:
        """
        ì—¬ëŸ¬ ëª¨ë¸ ì •í™•ë„ ë¹„êµ

        Args:
            other_model_paths: ë¹„êµí•  ë‹¤ë¥¸ ëª¨ë¸ë“¤
            device: GPU ë””ë°”ì´ìŠ¤

        Returns:
            Dict: ëª¨ë¸ ë¹„êµ ê²°ê³¼
        """
        logger.info(f"ğŸ† ëª¨ë¸ ì •í™•ë„ ë¹„êµ ì‹œì‘")

        comparison_results = {}

        # í˜„ì¬ ëª¨ë¸
        logger.info(f"ğŸ“Š ê¸°ì¤€ ëª¨ë¸: {self.model_path.name}")
        base_metrics = self.run_validation(device=device, plots=False, verbose=False)

        if base_metrics:
            comparison_results[self.model_path.name] = {
                'model_path': str(self.model_path),
                'model_type': self.model_type,
                'metrics': base_metrics
            }

        # ë‹¤ë¥¸ ëª¨ë¸ë“¤
        for model_path in other_model_paths:
            model_path = Path(model_path)
            logger.info(f"ğŸ“Š ë¹„êµ ëª¨ë¸: {model_path.name}")

            try:
                # ì„ì‹œë¡œ ëª¨ë¸ ê²½ë¡œ ë³€ê²½
                original_model = self.model_path
                original_type = self.model_type

                self.model_path = model_path
                self.model_type = self._determine_model_type()

                metrics = self.run_validation(device=device, plots=False, verbose=False)

                if metrics:
                    comparison_results[model_path.name] = {
                        'model_path': str(model_path),
                        'model_type': self.model_type,
                        'metrics': metrics
                    }

                # ì›ë˜ ëª¨ë¸ë¡œ ë³µì›
                self.model_path = original_model
                self.model_type = original_type

            except Exception as e:
                logger.warning(f"ëª¨ë¸ {model_path.name} ë¹„êµ ì‹¤íŒ¨: {e}")
                continue

        return comparison_results

    def _print_accuracy_summary(self, metrics: Dict):
        """ì •í™•ë„ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ¯ ì •í™•ë„ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼")
        print("="*60)
        print(f"ğŸ¤– ëª¨ë¸: {self.model_path.name}")
        print(f"ğŸ“Š ëª¨ë¸ íƒ€ì…: {self.model_type}")
        print("")

        print("ğŸ“ˆ ì •í™•ë„ ë©”íŠ¸ë¦­:")
        print(f"   mAP@0.5:            {metrics['map50']:.4f} ({metrics['map50']*100:.2f}%)")
        print(f"   mAP@0.5:0.95:       {metrics['map50_95']:.4f} ({metrics['map50_95']*100:.2f}%)")
        print(f"   ì •ë°€ë„ (Precision): {metrics['precision']:.4f} ({metrics['precision']*100:.2f}%)")
        print(f"   ì¬í˜„ìœ¨ (Recall):    {metrics['recall']:.4f} ({metrics['recall']*100:.2f}%)")
        print(f"   F1 ì ìˆ˜:           {metrics['f1_score']:.4f} ({metrics['f1_score']*100:.2f}%)")
        print("")

        print("â±ï¸  ê²€ì¦ ì‹œê°„:")
        print(f"   ì´ ê²€ì¦ ì‹œê°„:       {metrics['validation_time']:.2f}s")
        print("")

        # í´ë˜ìŠ¤ë³„ ê²°ê³¼ (ìˆëŠ” ê²½ìš°)
        if 'class_metrics' in metrics:
            print("ğŸ“‹ í´ë˜ìŠ¤ë³„ mAP@0.5:")
            for class_name, class_metric in metrics['class_metrics'].items():
                print(f"   {class_name}: {class_metric['map50']:.4f} ({class_metric['map50']*100:.2f}%)")
            print("")

        # ì„±ëŠ¥ í‰ê°€
        map_score = metrics['map50']
        if map_score >= 0.7:
            rating = "ğŸ”¥ ë§¤ìš° ìš°ìˆ˜"
        elif map_score >= 0.5:
            rating = "ğŸš€ ìš°ìˆ˜"
        elif map_score >= 0.3:
            rating = "âœ… ì–‘í˜¸"
        elif map_score >= 0.1:
            rating = "ğŸ“Š ë³´í†µ"
        else:
            rating = "âš ï¸ ê°œì„  í•„ìš”"

        print(f"ğŸ† ì¢…í•© í‰ê°€: {rating} (mAP@0.5: {map_score:.4f})")
        print("="*60)

    def save_results(self, results: Dict, output_dir: str = './accuracy_results',
                    custom_name: Optional[str] = None):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        # íŒŒì¼ëª… ìƒì„±
        if custom_name:
            filename = f"accuracy_{custom_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        else:
            model_name = self.model_path.stem.replace('.', '_')
            filename = f"accuracy_{model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        output_path = output_dir / filename

        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        results_with_meta = {
            'model_info': {
                'model_path': str(self.model_path),
                'model_name': self.model_path.name,
                'model_type': self.model_type,
                'file_size': self.model_path.stat().st_size
            },
            'dataset_info': {
                'data_yaml': str(self.data_yaml)
            },
            'system_info': {
                'platform': platform.system(),
                'timestamp': datetime.now().isoformat()
            },
            'results': results
        }

        # JSON ì €ì¥
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results_with_meta, f, indent=2, ensure_ascii=False, default=str)

        logger.info(f"ğŸ’¾ ì •í™•ë„ ê²°ê³¼ ì €ì¥: {output_path}")
        return output_path


def main():
    parser = argparse.ArgumentParser(description="ëª¨ë¸ ì •í™•ë„ ë²¤ì¹˜ë§ˆí¬")
    parser.add_argument('data', help='ë°ì´í„°ì…‹ YAML íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('model', help='ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (.engine, .pt, .onnx)')

    # ê²€ì¦ ì˜µì…˜
    parser.add_argument('--device', default='0', help='GPU ë””ë°”ì´ìŠ¤ (ê¸°ë³¸ê°’: 0)')
    parser.add_argument('--conf', type=float, default=0.001, help='ì‹ ë¢°ë„ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.001)')
    parser.add_argument('--iou', type=float, default=0.6, help='NMS IOU ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.6)')
    parser.add_argument('--imgsz', type=int, default=640, help='ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸° (ê¸°ë³¸ê°’: 640)')
    parser.add_argument('--max-det', type=int, default=300, help='ìµœëŒ€ ê²€ì¶œ ìˆ˜ (ê¸°ë³¸ê°’: 300)')

    # í…ŒìŠ¤íŠ¸ ì˜µì…˜
    parser.add_argument('--speed-vs-accuracy', action='store_true', help='ì†ë„ vs ì •í™•ë„ í…ŒìŠ¤íŠ¸')
    parser.add_argument('--compare-models', nargs='+', help='ë¹„êµí•  ë‹¤ë¥¸ ëª¨ë¸ë“¤')

    # ì¶œë ¥ ì˜µì…˜
    parser.add_argument('--output-dir', default='./accuracy_results', help='ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--name', help='ê²°ê³¼ íŒŒì¼ ì»¤ìŠ¤í…€ ì´ë¦„')
    parser.add_argument('--save-results', action='store_true', help='ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥')
    parser.add_argument('--no-plots', action='store_true', help='ì‹œê°í™” ì°¨íŠ¸ ìƒì„± ì•ˆí•¨')

    args = parser.parse_args()

    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not Path(args.data).exists():
        logger.error(f"âŒ ë°ì´í„°ì…‹ YAMLì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.data}")
        sys.exit(1)

    if not Path(args.model).exists():
        logger.error(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.model}")
        sys.exit(1)

    try:
        # ì •í™•ë„ ë²¤ì¹˜ë§ˆí¬ ì´ˆê¸°í™”
        benchmark = AccuracyBenchmark(args.data, args.model)

        # ê¸°ë³¸ ê²€ì¦ ì‹¤í–‰
        logger.info("ğŸ¯ ê¸°ë³¸ ì •í™•ë„ ê²€ì¦ ì‹¤í–‰ ì¤‘...")
        results = benchmark.run_validation(
            device=args.device,
            conf_threshold=args.conf,
            iou_threshold=args.iou,
            imgsz=args.imgsz,
            max_det=args.max_det,
            plots=not args.no_plots,
            save_dir=args.output_dir if args.save_results else None
        )

        if results is None:
            logger.error("âŒ ê¸°ë³¸ ê²€ì¦ ì‹¤íŒ¨")
            sys.exit(1)

        final_results = {'basic_validation': results}

        # ì†ë„ vs ì •í™•ë„ í…ŒìŠ¤íŠ¸
        if args.speed_vs_accuracy:
            logger.info("âš–ï¸  ì†ë„ vs ì •í™•ë„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
            speed_accuracy_results = benchmark.run_speed_vs_accuracy_test(device=args.device)
            final_results['speed_vs_accuracy'] = speed_accuracy_results

        # ëª¨ë¸ ë¹„êµ
        if args.compare_models:
            logger.info("ğŸ† ëª¨ë¸ ë¹„êµ ì‹¤í–‰ ì¤‘...")
            comparison_results = benchmark.compare_models(args.compare_models, device=args.device)
            final_results['model_comparison'] = comparison_results

        # ê²°ê³¼ ì €ì¥
        if args.save_results:
            benchmark.save_results(final_results, args.output_dir, args.name)

        logger.info("ğŸ‰ ì •í™•ë„ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")

    except KeyboardInterrupt:
        logger.info("âŒ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()